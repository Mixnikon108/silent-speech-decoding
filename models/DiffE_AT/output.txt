[config] Parámetros: Namespace(dataset_file='/media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/data/processed/BCI2020/BCI_processed.npz', dataset='BCI', device='cuda:0', num_epochs=100, batch_train=250, batch_eval=32, seed=42, alpha=1.0, subject_id=2, num_classes=5, channels=64, n_T=100, ddpm_dim=128, encoder_dim=64, fc_dim=64, exp_dir='/media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_AT')
[setup] Semilla fijada a 42
[setup] Usando dispositivo: cuda:0
[data] Cargando datos del dataset BCI
[INFO] Cargando datos desde: /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/data/processed/BCI2020/BCI_processed.npz
[INFO] Aplicando z-score normalization por trial
    - Forma de entrada (esperada): (trials, canales, muestras) = torch.Size([4500, 64, 795])
    - Calculando media y desviación estándar por trial
    - Mean shape: torch.Size([4500, 1, 1]), Std shape: torch.Size([4500, 1, 1])
    - Normalización completada. Forma de salida: torch.Size([4500, 64, 795])
[INFO] Padding aplicado: se añaden 5 valores a la derecha (dim original: 795)
[INFO] Dimensión temporal final: 800
[INFO] Aplicando z-score normalization por trial
    - Forma de entrada (esperada): (trials, canales, muestras) = torch.Size([750, 64, 795])
    - Calculando media y desviación estándar por trial
    - Mean shape: torch.Size([750, 1, 1]), Std shape: torch.Size([750, 1, 1])
    - Normalización completada. Forma de salida: torch.Size([750, 64, 795])
[INFO] Padding aplicado: se añaden 5 valores a la derecha (dim original: 795)
[INFO] Dimensión temporal final: 800
[INFO] Aplicando z-score normalization por trial
    - Forma de entrada (esperada): (trials, canales, muestras) = torch.Size([750, 64, 795])
    - Calculando media y desviación estándar por trial
    - Mean shape: torch.Size([750, 1, 1]), Std shape: torch.Size([750, 1, 1])
    - Normalización completada. Forma de salida: torch.Size([750, 64, 795])
[INFO] Padding aplicado: se añaden 5 valores a la derecha (dim original: 795)
[INFO] Dimensión temporal final: 800
[INFO] Filtrando datos para el sujeto 2 (índice interno: 1)
[INFO] Seleccionando idx slice(300, 600, None) para X_train, slice(50, 100, None) para X_val, slice(50, 100, None) para X_test
[INFO] Dimensiones finales:
    - X_train: torch.Size([300, 64, 800]), y_train: torch.Size([300])
    - X_val:   torch.Size([50, 64, 800]), y_val:   torch.Size([50])
    - X_test:  torch.Size([50, 64, 800]), y_test:  torch.Size([50])
[INFO] Creando DataLoaders:
    - Batch size (train): 250  | Shuffle: True
    - Batch size (eval) : 32  | Shuffle: False
[INFO] Número de batches:
    - Train: 2 batches
    - Val  : 2 batches
    - Test : 2 batches
[model] Inicializando modelos...
[model] FLOPs: 0.44 GFLOPs | Peso: 12.53 MB | Parámetros: 3.29 M
[setup] Configurando optimizadores y schedulers: base_lr=0.0003, max_lr=0.003, step_size=50
[setup] EMA configurada en el clasificatorio (beta=0.95)
[train] Inicio del bucle de entrenamiento por 100 épocas

[train] ===== Época 1/100 =====
[train] acc: 0.236 | Loss DDPM: 0.75710 | Loss Gap: 0.75601 | Loss Cls: 1.79378
[eval] Epoch 1 — acc: 0.2200, macro_f1: 0.1640
[checkpoint] ¡Nueva mejor accuracy: 0.2200! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_AT/best.pt

[train] ===== Época 2/100 =====
[train] acc: 0.218 | Loss DDPM: 0.66839 | Loss Gap: 0.53136 | Loss Cls: 1.71015
[eval] Epoch 2 — acc: 0.2200, macro_f1: 0.1584

[train] ===== Época 3/100 =====
[train] acc: 0.280 | Loss DDPM: 0.61248 | Loss Gap: 0.43701 | Loss Cls: 1.65161
[eval] Epoch 3 — acc: 0.1600, macro_f1: 0.1331

[train] ===== Época 4/100 =====
[train] acc: 0.290 | Loss DDPM: 0.58256 | Loss Gap: 0.38589 | Loss Cls: 1.63436
[eval] Epoch 4 — acc: 0.2800, macro_f1: 0.2807
[checkpoint] ¡Nueva mejor accuracy: 0.2800! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_AT/best.pt

[train] ===== Época 5/100 =====
[train] acc: 0.296 | Loss DDPM: 0.54705 | Loss Gap: 0.36838 | Loss Cls: 1.55625
[eval] Epoch 5 — acc: 0.2000, macro_f1: 0.2018

[train] ===== Época 6/100 =====
[train] acc: 0.288 | Loss DDPM: 0.53289 | Loss Gap: 0.36456 | Loss Cls: 1.62512
[eval] Epoch 6 — acc: 0.2000, macro_f1: 0.2033

[train] ===== Época 7/100 =====
[train] acc: 0.286 | Loss DDPM: 0.47946 | Loss Gap: 0.34367 | Loss Cls: 1.68992
[eval] Epoch 7 — acc: 0.1400, macro_f1: 0.1411

[train] ===== Época 8/100 =====
[train] acc: 0.272 | Loss DDPM: 0.48602 | Loss Gap: 0.35379 | Loss Cls: 1.57619
[eval] Epoch 8 — acc: 0.1200, macro_f1: 0.0882

[train] ===== Época 9/100 =====
[train] acc: 0.330 | Loss DDPM: 0.48288 | Loss Gap: 0.33697 | Loss Cls: 1.59114
[eval] Epoch 9 — acc: 0.1800, macro_f1: 0.1332

[train] ===== Época 10/100 =====
[train] acc: 0.300 | Loss DDPM: 0.49773 | Loss Gap: 0.34482 | Loss Cls: 1.61305
[eval] Epoch 10 — acc: 0.1800, macro_f1: 0.1353

[train] ===== Época 11/100 =====
[train] acc: 0.344 | Loss DDPM: 0.45366 | Loss Gap: 0.31548 | Loss Cls: 1.57411
[eval] Epoch 11 — acc: 0.1600, macro_f1: 0.1400

[train] ===== Época 12/100 =====
[train] acc: 0.306 | Loss DDPM: 0.43989 | Loss Gap: 0.33319 | Loss Cls: 1.59427
[eval] Epoch 12 — acc: 0.1000, macro_f1: 0.0891

[train] ===== Época 13/100 =====
[train] acc: 0.342 | Loss DDPM: 0.45107 | Loss Gap: 0.35054 | Loss Cls: 1.61363
[eval] Epoch 13 — acc: 0.1600, macro_f1: 0.1482

[train] ===== Época 14/100 =====
[train] acc: 0.256 | Loss DDPM: 0.42493 | Loss Gap: 0.30754 | Loss Cls: 1.55660
[eval] Epoch 14 — acc: 0.1600, macro_f1: 0.1352

[train] ===== Época 15/100 =====
[train] acc: 0.316 | Loss DDPM: 0.43389 | Loss Gap: 0.31690 | Loss Cls: 1.52706
[eval] Epoch 15 — acc: 0.1400, macro_f1: 0.1376

[train] ===== Época 16/100 =====
[train] acc: 0.332 | Loss DDPM: 0.41719 | Loss Gap: 0.32175 | Loss Cls: 1.60425
[eval] Epoch 16 — acc: 0.1400, macro_f1: 0.1048

[train] ===== Época 17/100 =====
[train] acc: 0.336 | Loss DDPM: 0.43479 | Loss Gap: 0.32160 | Loss Cls: 1.55895
[eval] Epoch 17 — acc: 0.2200, macro_f1: 0.1745

[train] ===== Época 18/100 =====
[train] acc: 0.372 | Loss DDPM: 0.41474 | Loss Gap: 0.32878 | Loss Cls: 1.55559
[eval] Epoch 18 — acc: 0.1400, macro_f1: 0.0979

[train] ===== Época 19/100 =====
[train] acc: 0.412 | Loss DDPM: 0.41756 | Loss Gap: 0.32601 | Loss Cls: 1.49938
[eval] Epoch 19 — acc: 0.1800, macro_f1: 0.1017

[train] ===== Época 20/100 =====
[train] acc: 0.478 | Loss DDPM: 0.41123 | Loss Gap: 0.34134 | Loss Cls: 1.49430
[eval] Epoch 20 — acc: 0.1200, macro_f1: 0.0826

[train] ===== Época 21/100 =====
[train] acc: 0.422 | Loss DDPM: 0.39123 | Loss Gap: 0.31988 | Loss Cls: 1.47036
[eval] Epoch 21 — acc: 0.2200, macro_f1: 0.1263

[train] ===== Época 22/100 =====
[train] acc: 0.448 | Loss DDPM: 0.39925 | Loss Gap: 0.34160 | Loss Cls: 1.50864
[eval] Epoch 22 — acc: 0.1400, macro_f1: 0.1121

[train] ===== Época 23/100 =====
[train] acc: 0.392 | Loss DDPM: 0.41385 | Loss Gap: 0.33568 | Loss Cls: 1.49960
[eval] Epoch 23 — acc: 0.1600, macro_f1: 0.1468

[train] ===== Época 24/100 =====
[train] acc: 0.484 | Loss DDPM: 0.39540 | Loss Gap: 0.35939 | Loss Cls: 1.43194
[eval] Epoch 24 — acc: 0.2400, macro_f1: 0.1878

[train] ===== Época 25/100 =====
[train] acc: 0.464 | Loss DDPM: 0.43020 | Loss Gap: 0.46144 | Loss Cls: 1.55093
[eval] Epoch 25 — acc: 0.1800, macro_f1: 0.1047

[train] ===== Época 26/100 =====
[train] acc: 0.454 | Loss DDPM: 0.39535 | Loss Gap: 0.39207 | Loss Cls: 1.52502
[eval] Epoch 26 — acc: 0.1800, macro_f1: 0.1311

[train] ===== Época 27/100 =====
[train] acc: 0.438 | Loss DDPM: 0.41244 | Loss Gap: 0.33568 | Loss Cls: 1.53483
[eval] Epoch 27 — acc: 0.1200, macro_f1: 0.0661

[train] ===== Época 28/100 =====
[train] acc: 0.478 | Loss DDPM: 0.38406 | Loss Gap: 0.29490 | Loss Cls: 1.43606
[eval] Epoch 28 — acc: 0.1400, macro_f1: 0.0960

[train] ===== Época 29/100 =====
[train] acc: 0.406 | Loss DDPM: 0.38798 | Loss Gap: 0.31740 | Loss Cls: 1.48933
[eval] Epoch 29 — acc: 0.2000, macro_f1: 0.1487

[train] ===== Época 30/100 =====
[train] acc: 0.406 | Loss DDPM: 0.39231 | Loss Gap: 0.32319 | Loss Cls: 1.46712
[eval] Epoch 30 — acc: 0.1800, macro_f1: 0.1491

[train] ===== Época 31/100 =====
[train] acc: 0.498 | Loss DDPM: 0.39182 | Loss Gap: 0.28941 | Loss Cls: 1.43123
[eval] Epoch 31 — acc: 0.2000, macro_f1: 0.2081

[train] ===== Época 32/100 =====
[train] acc: 0.486 | Loss DDPM: 0.40435 | Loss Gap: 0.28531 | Loss Cls: 1.45117
[eval] Epoch 32 — acc: 0.1200, macro_f1: 0.1036

[train] ===== Época 33/100 =====
[train] acc: 0.490 | Loss DDPM: 0.37986 | Loss Gap: 0.26200 | Loss Cls: 1.47932
[eval] Epoch 33 — acc: 0.1800, macro_f1: 0.1758

[train] ===== Época 34/100 =====
[train] acc: 0.526 | Loss DDPM: 0.37977 | Loss Gap: 0.25675 | Loss Cls: 1.34096
[eval] Epoch 34 — acc: 0.1200, macro_f1: 0.0927

[train] ===== Época 35/100 =====
[train] acc: 0.540 | Loss DDPM: 0.37893 | Loss Gap: 0.25711 | Loss Cls: 1.38581
[eval] Epoch 35 — acc: 0.1800, macro_f1: 0.1758

[train] ===== Época 36/100 =====
[train] acc: 0.520 | Loss DDPM: 0.37456 | Loss Gap: 0.25295 | Loss Cls: 1.31924
[eval] Epoch 36 — acc: 0.1200, macro_f1: 0.0865

[train] ===== Época 37/100 =====
[train] acc: 0.570 | Loss DDPM: 0.35905 | Loss Gap: 0.24086 | Loss Cls: 1.28678
[eval] Epoch 37 — acc: 0.1800, macro_f1: 0.1435

[train] ===== Época 38/100 =====
[train] acc: 0.558 | Loss DDPM: 0.37220 | Loss Gap: 0.24633 | Loss Cls: 1.38099
[eval] Epoch 38 — acc: 0.0800, macro_f1: 0.0614

[train] ===== Época 39/100 =====
[train] acc: 0.570 | Loss DDPM: 0.35874 | Loss Gap: 0.23325 | Loss Cls: 1.24329
[eval] Epoch 39 — acc: 0.1000, macro_f1: 0.1141

[train] ===== Época 40/100 =====
[train] acc: 0.530 | Loss DDPM: 0.35753 | Loss Gap: 0.23223 | Loss Cls: 1.27024
[eval] Epoch 40 — acc: 0.2000, macro_f1: 0.1479

[train] ===== Época 41/100 =====
[train] acc: 0.582 | Loss DDPM: 0.35997 | Loss Gap: 0.23168 | Loss Cls: 1.27709
[eval] Epoch 41 — acc: 0.1600, macro_f1: 0.1614

[train] ===== Época 42/100 =====
[train] acc: 0.584 | Loss DDPM: 0.35677 | Loss Gap: 0.23018 | Loss Cls: 1.27937
[eval] Epoch 42 — acc: 0.0600, macro_f1: 0.0659

[train] ===== Época 43/100 =====
[train] acc: 0.578 | Loss DDPM: 0.37417 | Loss Gap: 0.23892 | Loss Cls: 1.22917
[eval] Epoch 43 — acc: 0.1400, macro_f1: 0.1385

[train] ===== Época 44/100 =====
[train] acc: 0.600 | Loss DDPM: 0.36602 | Loss Gap: 0.23291 | Loss Cls: 1.14465
[eval] Epoch 44 — acc: 0.1400, macro_f1: 0.0922

[train] ===== Época 45/100 =====
[train] acc: 0.624 | Loss DDPM: 0.35584 | Loss Gap: 0.22688 | Loss Cls: 1.17290
[eval] Epoch 45 — acc: 0.1600, macro_f1: 0.1369

[train] ===== Época 46/100 =====
[train] acc: 0.678 | Loss DDPM: 0.36877 | Loss Gap: 0.23430 | Loss Cls: 1.15711
[eval] Epoch 46 — acc: 0.1600, macro_f1: 0.1172

[train] ===== Época 47/100 =====
[train] acc: 0.688 | Loss DDPM: 0.35115 | Loss Gap: 0.22318 | Loss Cls: 1.10870
[eval] Epoch 47 — acc: 0.1200, macro_f1: 0.1121

[train] ===== Época 48/100 =====
[train] acc: 0.664 | Loss DDPM: 0.34339 | Loss Gap: 0.21795 | Loss Cls: 1.01695
[eval] Epoch 48 — acc: 0.1200, macro_f1: 0.1030

[train] ===== Época 49/100 =====
[train] acc: 0.682 | Loss DDPM: 0.35358 | Loss Gap: 0.22376 | Loss Cls: 1.05012
[eval] Epoch 49 — acc: 0.1200, macro_f1: 0.1021

[train] ===== Época 50/100 =====
[train] acc: 0.654 | Loss DDPM: 0.35080 | Loss Gap: 0.22250 | Loss Cls: 1.02357
[eval] Epoch 50 — acc: 0.1000, macro_f1: 0.0949

[train] ===== Época 51/100 =====
[train] acc: 0.656 | Loss DDPM: 0.37801 | Loss Gap: 0.23930 | Loss Cls: 1.03477
[eval] Epoch 51 — acc: 0.1600, macro_f1: 0.1656

[train] ===== Época 52/100 =====
[train] acc: 0.680 | Loss DDPM: 0.36530 | Loss Gap: 0.23045 | Loss Cls: 1.03213
[eval] Epoch 52 — acc: 0.2000, macro_f1: 0.2004

[train] ===== Época 53/100 =====
[train] acc: 0.700 | Loss DDPM: 0.35652 | Loss Gap: 0.22765 | Loss Cls: 1.01993
[eval] Epoch 53 — acc: 0.1400, macro_f1: 0.1303

[train] ===== Época 54/100 =====
[train] acc: 0.652 | Loss DDPM: 0.35983 | Loss Gap: 0.22804 | Loss Cls: 1.05192
[eval] Epoch 54 — acc: 0.2000, macro_f1: 0.1828

[train] ===== Época 55/100 =====
[train] acc: 0.698 | Loss DDPM: 0.33879 | Loss Gap: 0.21553 | Loss Cls: 1.04323
[eval] Epoch 55 — acc: 0.1800, macro_f1: 0.1715

[train] ===== Época 56/100 =====
[train] acc: 0.674 | Loss DDPM: 0.37679 | Loss Gap: 0.24114 | Loss Cls: 0.99796
[eval] Epoch 56 — acc: 0.1600, macro_f1: 0.1614

[train] ===== Época 57/100 =====
[train] acc: 0.590 | Loss DDPM: 0.36749 | Loss Gap: 0.23277 | Loss Cls: 1.09521
[eval] Epoch 57 — acc: 0.1600, macro_f1: 0.1581

[train] ===== Época 58/100 =====
[train] acc: 0.718 | Loss DDPM: 0.34431 | Loss Gap: 0.21897 | Loss Cls: 1.08897
[eval] Epoch 58 — acc: 0.2000, macro_f1: 0.1871

[train] ===== Época 59/100 =====
[train] acc: 0.684 | Loss DDPM: 0.34997 | Loss Gap: 0.22245 | Loss Cls: 1.02642
[eval] Epoch 59 — acc: 0.2200, macro_f1: 0.2201

[train] ===== Época 60/100 =====
[train] acc: 0.710 | Loss DDPM: 0.39154 | Loss Gap: 0.25208 | Loss Cls: 1.00157
[eval] Epoch 60 — acc: 0.1400, macro_f1: 0.1260

[train] ===== Época 61/100 =====
[train] acc: 0.646 | Loss DDPM: 0.36464 | Loss Gap: 0.23529 | Loss Cls: 1.04933
[eval] Epoch 61 — acc: 0.2000, macro_f1: 0.1758

[train] ===== Época 62/100 =====
[train] acc: 0.730 | Loss DDPM: 0.36642 | Loss Gap: 0.23813 | Loss Cls: 1.13491
[eval] Epoch 62 — acc: 0.1200, macro_f1: 0.0571

[train] ===== Época 63/100 =====
[train] acc: 0.594 | Loss DDPM: 0.36228 | Loss Gap: 0.23762 | Loss Cls: 1.14087
[eval] Epoch 63 — acc: 0.2400, macro_f1: 0.1747

[train] ===== Época 64/100 =====
[train] acc: 0.742 | Loss DDPM: 0.37591 | Loss Gap: 0.25104 | Loss Cls: 0.97231
[eval] Epoch 64 — acc: 0.2200, macro_f1: 0.2237

[train] ===== Época 65/100 =====
[train] acc: 0.594 | Loss DDPM: 0.35738 | Loss Gap: 0.24448 | Loss Cls: 1.12384
[eval] Epoch 65 — acc: 0.2400, macro_f1: 0.2353

[train] ===== Época 66/100 =====
[train] acc: 0.664 | Loss DDPM: 0.36274 | Loss Gap: 0.25145 | Loss Cls: 1.16271
[eval] Epoch 66 — acc: 0.3000, macro_f1: 0.2638
[checkpoint] ¡Nueva mejor accuracy: 0.3000! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_AT/best.pt

[train] ===== Época 67/100 =====
[train] acc: 0.654 | Loss DDPM: 0.33734 | Loss Gap: 0.25594 | Loss Cls: 1.06193
[eval] Epoch 67 — acc: 0.1800, macro_f1: 0.0923

[train] ===== Época 68/100 =====
[train] acc: 0.662 | Loss DDPM: 0.35137 | Loss Gap: 0.28104 | Loss Cls: 1.14003
[eval] Epoch 68 — acc: 0.1400, macro_f1: 0.0744

[train] ===== Época 69/100 =====
[train] acc: 0.608 | Loss DDPM: 0.37136 | Loss Gap: 0.27602 | Loss Cls: 1.12659
[eval] Epoch 69 — acc: 0.2400, macro_f1: 0.1794

[train] ===== Época 70/100 =====
[train] acc: 0.624 | Loss DDPM: 0.36953 | Loss Gap: 0.26170 | Loss Cls: 1.08353
[eval] Epoch 70 — acc: 0.1400, macro_f1: 0.0583

[train] ===== Época 71/100 =====
[train] acc: 0.764 | Loss DDPM: 0.36091 | Loss Gap: 0.27661 | Loss Cls: 1.01526
[eval] Epoch 71 — acc: 0.2000, macro_f1: 0.1480

[train] ===== Época 72/100 =====
[train] acc: 0.622 | Loss DDPM: 0.37160 | Loss Gap: 0.26730 | Loss Cls: 0.93667
[eval] Epoch 72 — acc: 0.2200, macro_f1: 0.1130

[train] ===== Época 73/100 =====
[train] acc: 0.666 | Loss DDPM: 0.38171 | Loss Gap: 0.26289 | Loss Cls: 1.23838
[eval] Epoch 73 — acc: 0.2200, macro_f1: 0.1510

[train] ===== Época 74/100 =====
[train] acc: 0.708 | Loss DDPM: 0.36056 | Loss Gap: 0.25159 | Loss Cls: 0.95636
[eval] Epoch 74 — acc: 0.1400, macro_f1: 0.1043

[train] ===== Época 75/100 =====
[train] acc: 0.692 | Loss DDPM: 0.35085 | Loss Gap: 0.26975 | Loss Cls: 1.03732
[eval] Epoch 75 — acc: 0.2400, macro_f1: 0.2218

[train] ===== Época 76/100 =====
[train] acc: 0.634 | Loss DDPM: 0.38580 | Loss Gap: 0.28386 | Loss Cls: 1.10033
[eval] Epoch 76 — acc: 0.1400, macro_f1: 0.1229

[train] ===== Época 77/100 =====
[train] acc: 0.654 | Loss DDPM: 0.35864 | Loss Gap: 0.25205 | Loss Cls: 0.95259
[eval] Epoch 77 — acc: 0.2200, macro_f1: 0.1842

[train] ===== Época 78/100 =====
[train] acc: 0.664 | Loss DDPM: 0.35942 | Loss Gap: 0.27055 | Loss Cls: 1.01953
[eval] Epoch 78 — acc: 0.1800, macro_f1: 0.0843

[train] ===== Época 79/100 =====
[train] acc: 0.724 | Loss DDPM: 0.34723 | Loss Gap: 0.24804 | Loss Cls: 1.02228
[eval] Epoch 79 — acc: 0.2000, macro_f1: 0.1670

[train] ===== Época 80/100 =====
[train] acc: 0.714 | Loss DDPM: 0.32320 | Loss Gap: 0.24587 | Loss Cls: 0.91358
[eval] Epoch 80 — acc: 0.2000, macro_f1: 0.1215

[train] ===== Época 81/100 =====
[train] acc: 0.788 | Loss DDPM: 0.36116 | Loss Gap: 0.25291 | Loss Cls: 0.93176
[eval] Epoch 81 — acc: 0.1400, macro_f1: 0.1144

[train] ===== Época 82/100 =====
[train] acc: 0.756 | Loss DDPM: 0.33929 | Loss Gap: 0.24057 | Loss Cls: 0.86664
[eval] Epoch 82 — acc: 0.2200, macro_f1: 0.1965

[train] ===== Época 83/100 =====
[train] acc: 0.748 | Loss DDPM: 0.33593 | Loss Gap: 0.24106 | Loss Cls: 0.95325
[eval] Epoch 83 — acc: 0.1800, macro_f1: 0.1504

[train] ===== Época 84/100 =====
[train] acc: 0.764 | Loss DDPM: 0.34954 | Loss Gap: 0.23981 | Loss Cls: 1.14900
[eval] Epoch 84 — acc: 0.2200, macro_f1: 0.1892

[train] ===== Época 85/100 =====
[train] acc: 0.738 | Loss DDPM: 0.35400 | Loss Gap: 0.23671 | Loss Cls: 0.99568
[eval] Epoch 85 — acc: 0.2200, macro_f1: 0.1680

[train] ===== Época 86/100 =====
[train] acc: 0.812 | Loss DDPM: 0.33054 | Loss Gap: 0.22064 | Loss Cls: 0.85862
[eval] Epoch 86 — acc: 0.2000, macro_f1: 0.1886

[train] ===== Época 87/100 =====
[train] acc: 0.792 | Loss DDPM: 0.37853 | Loss Gap: 0.24662 | Loss Cls: 0.67845
[eval] Epoch 87 — acc: 0.2000, macro_f1: 0.1769

[train] ===== Época 88/100 =====
[train] acc: 0.826 | Loss DDPM: 0.35329 | Loss Gap: 0.22380 | Loss Cls: 0.69471
[eval] Epoch 88 — acc: 0.1400, macro_f1: 0.1379

[train] ===== Época 89/100 =====
[train] acc: 0.818 | Loss DDPM: 0.34159 | Loss Gap: 0.21491 | Loss Cls: 0.78990
[eval] Epoch 89 — acc: 0.1800, macro_f1: 0.1685

[train] ===== Época 90/100 =====
[train] acc: 0.786 | Loss DDPM: 0.32421 | Loss Gap: 0.20472 | Loss Cls: 0.66614
[eval] Epoch 90 — acc: 0.1800, macro_f1: 0.1764

[train] ===== Época 91/100 =====
[train] acc: 0.872 | Loss DDPM: 0.35250 | Loss Gap: 0.22206 | Loss Cls: 0.62039
[eval] Epoch 91 — acc: 0.1800, macro_f1: 0.1679

[train] ===== Época 92/100 =====
[train] acc: 0.852 | Loss DDPM: 0.33851 | Loss Gap: 0.21187 | Loss Cls: 0.55978
[eval] Epoch 92 — acc: 0.1400, macro_f1: 0.1099

[train] ===== Época 93/100 =====
[train] acc: 0.908 | Loss DDPM: 0.34858 | Loss Gap: 0.21802 | Loss Cls: 0.56826
[eval] Epoch 93 — acc: 0.1600, macro_f1: 0.1477

[train] ===== Época 94/100 =====
[train] acc: 0.900 | Loss DDPM: 0.31848 | Loss Gap: 0.19821 | Loss Cls: 0.46821
[eval] Epoch 94 — acc: 0.1600, macro_f1: 0.1407

[train] ===== Época 95/100 =====
[train] acc: 0.840 | Loss DDPM: 0.32006 | Loss Gap: 0.19998 | Loss Cls: 0.52606
[eval] Epoch 95 — acc: 0.1400, macro_f1: 0.1283

[train] ===== Época 96/100 =====
[train] acc: 0.902 | Loss DDPM: 0.33483 | Loss Gap: 0.20867 | Loss Cls: 0.48518
[eval] Epoch 96 — acc: 0.1400, macro_f1: 0.1257

[train] ===== Época 97/100 =====
[train] acc: 0.930 | Loss DDPM: 0.34920 | Loss Gap: 0.21705 | Loss Cls: 0.43225
[eval] Epoch 97 — acc: 0.1800, macro_f1: 0.1680

[train] ===== Época 98/100 =====
[train] acc: 0.902 | Loss DDPM: 0.31458 | Loss Gap: 0.19478 | Loss Cls: 0.47083
[eval] Epoch 98 — acc: 0.1600, macro_f1: 0.1507

[train] ===== Época 99/100 =====
[train] acc: 0.884 | Loss DDPM: 0.34609 | Loss Gap: 0.21461 | Loss Cls: 0.43948
[eval] Epoch 99 — acc: 0.1200, macro_f1: 0.1187

[train] ===== Época 100/100 =====
[train] acc: 0.918 | Loss DDPM: 0.30607 | Loss Gap: 0.19029 | Loss Cls: 0.40436
[eval] Epoch 100 — acc: 0.1200, macro_f1: 0.1088

[test] Evaluando el mejor modelo en el conjunto de test…
[test] Checkpoint restaurado desde /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_AT/best.pt
[test] Reporte completo:

  metrics:
            accuracy: 0.2400
    balanced_accuracy: 0.2400
            macro_f1: 0.2086
     macro_precision: 0.2213
        macro_recall: 0.2400
         roc_auc_ovo: 0.4750
                 mcc: 0.0543
         cohen_kappa: 0.0500
    confusion_matrix: [[2 0 5 2 1]
 [0 2 5 3 0]
 [1 1 6 1 1]
 [2 1 4 2 1]
 [1 2 5 2 0]]

  baseline_random:
            accuracy: 0.2000
    balanced_accuracy: 0.2028
            macro_f1: 0.1979
     macro_precision: 0.2018
        macro_recall: 0.2028
         roc_auc_ovo: 0.5000
                 mcc: 0.0035
         cohen_kappa: 0.0035

  improvement_%:
            accuracy: 20.0000
    balanced_accuracy: 18.3432
            macro_f1: 5.4035
     macro_precision: 9.6635
        macro_recall: 18.3432
         roc_auc_ovo: -5.0000
                 mcc: 1448.5986
         cohen_kappa: 1328.5714

  confusion_matrix:
[[2 0 5 2 1]
 [0 2 5 3 0]
 [1 1 6 1 1]
 [2 1 4 2 1]
 [1 2 5 2 0]]
