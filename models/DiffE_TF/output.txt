[config] Parámetros: Namespace(dataset_file='/media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/data/processed/BCI2020/BCI_raw.npz', dataset='BCI', device='cuda:0', num_epochs=100, batch_train=250, batch_eval=32, seed=42, alpha=10.0, subject_id=None, num_classes=5, channels=64, n_T=100, ddpm_dim=128, encoder_dim=64, fc_dim=64, exp_dir='/media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF')
[setup] Semilla fijada a 42
[setup] Usando dispositivo: cuda:0
[data] Cargando datos del dataset BCI
[INFO] Cargando datos desde: /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/data/processed/BCI2020/BCI_raw.npz
[INFO] Aplicando z-score normalization por trial
    - Forma de entrada (esperada): (trials, canales, muestras) = torch.Size([4500, 64, 795])
    - Calculando media y desviación estándar por trial
    - Mean shape: torch.Size([4500, 1, 1]), Std shape: torch.Size([4500, 1, 1])
    - Normalización completada. Forma de salida: torch.Size([4500, 64, 795])
[INFO] Padding aplicado: se añaden 5 valores a la derecha (dim original: 795)
[INFO] Dimensión temporal final: 800
[INFO] Aplicando z-score normalization por trial
    - Forma de entrada (esperada): (trials, canales, muestras) = torch.Size([750, 64, 795])
    - Calculando media y desviación estándar por trial
    - Mean shape: torch.Size([750, 1, 1]), Std shape: torch.Size([750, 1, 1])
    - Normalización completada. Forma de salida: torch.Size([750, 64, 795])
[INFO] Padding aplicado: se añaden 5 valores a la derecha (dim original: 795)
[INFO] Dimensión temporal final: 800
[INFO] Aplicando z-score normalization por trial
    - Forma de entrada (esperada): (trials, canales, muestras) = torch.Size([750, 64, 795])
    - Calculando media y desviación estándar por trial
    - Mean shape: torch.Size([750, 1, 1]), Std shape: torch.Size([750, 1, 1])
    - Normalización completada. Forma de salida: torch.Size([750, 64, 795])
[INFO] Padding aplicado: se añaden 5 valores a la derecha (dim original: 795)
[INFO] Dimensión temporal final: 800
[INFO] Cargando datos de todos los sujetos (concatenados)
[INFO] Dimensiones finales:
    - X_train: torch.Size([4500, 64, 800]), y_train: torch.Size([4500])
    - X_val:   torch.Size([750, 64, 800]), y_val:   torch.Size([750])
    - X_test:  torch.Size([750, 64, 800]), y_test:  torch.Size([750])
[INFO] Creando DataLoaders:
    - Batch size (train): 250  | Shuffle: True
    - Batch size (eval) : 32  | Shuffle: False
[INFO] Número de batches:
    - Train: 18 batches
    - Val  : 24 batches
    - Test : 24 batches
[model] Inicializando modelos...
[model] FLOPs: 0.02 GFLOPs | Peso: 3.21 MB | Parámetros: 0.84 M
[setup] Configurando optimizadores y schedulers: base_lr=0.0003, max_lr=0.003, step_size=50
[setup] EMA configurada en el clasificatorio (beta=0.95)
[train] Inicio del bucle de entrenamiento por 100 épocas

[train] ===== Época 1/100 =====
[train] acc: 0.241 | Loss DDPM: 0.63068 | Loss Gap: 0.59390 | Loss Cls: 1.67733
[eval] Epoch 1 — acc: 0.2240, macro_f1: 0.2059
[checkpoint] ¡Nueva mejor accuracy: 0.2240! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF/best.pt

[train] ===== Época 2/100 =====
[train] acc: 0.245 | Loss DDPM: 0.50157 | Loss Gap: 0.42681 | Loss Cls: 1.64503
[eval] Epoch 2 — acc: 0.2067, macro_f1: 0.1760

[train] ===== Época 3/100 =====
[train] acc: 0.251 | Loss DDPM: 0.46109 | Loss Gap: 0.37243 | Loss Cls: 1.62124
[eval] Epoch 3 — acc: 0.2293, macro_f1: 0.2247
[checkpoint] ¡Nueva mejor accuracy: 0.2293! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF/best.pt

[train] ===== Época 4/100 =====
[train] acc: 0.242 | Loss DDPM: 0.42331 | Loss Gap: 0.32262 | Loss Cls: 1.60680
[eval] Epoch 4 — acc: 0.2533, macro_f1: 0.2355
[checkpoint] ¡Nueva mejor accuracy: 0.2533! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF/best.pt

[train] ===== Época 5/100 =====
[train] acc: 0.263 | Loss DDPM: 0.39732 | Loss Gap: 0.29056 | Loss Cls: 1.59776
[eval] Epoch 5 — acc: 0.2640, macro_f1: 0.2517
[checkpoint] ¡Nueva mejor accuracy: 0.2640! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF/best.pt

[train] ===== Época 6/100 =====
[train] acc: 0.258 | Loss DDPM: 0.38979 | Loss Gap: 0.28035 | Loss Cls: 1.58548
[eval] Epoch 6 — acc: 0.2733, macro_f1: 0.2634
[checkpoint] ¡Nueva mejor accuracy: 0.2733! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF/best.pt

[train] ===== Época 7/100 =====
[train] acc: 0.287 | Loss DDPM: 0.39350 | Loss Gap: 0.27878 | Loss Cls: 1.58740
[eval] Epoch 7 — acc: 0.2307, macro_f1: 0.2145

[train] ===== Época 8/100 =====
[train] acc: 0.307 | Loss DDPM: 0.40799 | Loss Gap: 0.28429 | Loss Cls: 1.58974
[eval] Epoch 8 — acc: 0.2360, macro_f1: 0.2269

[train] ===== Época 9/100 =====
[train] acc: 0.309 | Loss DDPM: 0.40157 | Loss Gap: 0.27329 | Loss Cls: 1.58636
[eval] Epoch 9 — acc: 0.2387, macro_f1: 0.2205

[train] ===== Época 10/100 =====
[train] acc: 0.309 | Loss DDPM: 0.37900 | Loss Gap: 0.25534 | Loss Cls: 1.57921
[eval] Epoch 10 — acc: 0.2467, macro_f1: 0.2474

[train] ===== Época 11/100 =====
[train] acc: 0.312 | Loss DDPM: 0.36385 | Loss Gap: 0.24366 | Loss Cls: 1.56088
[eval] Epoch 11 — acc: 0.2680, macro_f1: 0.2631

[train] ===== Época 12/100 =====
[train] acc: 0.336 | Loss DDPM: 0.36342 | Loss Gap: 0.24345 | Loss Cls: 1.54288
[eval] Epoch 12 — acc: 0.2840, macro_f1: 0.2789
[checkpoint] ¡Nueva mejor accuracy: 0.2840! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF/best.pt

[train] ===== Época 13/100 =====
[train] acc: 0.354 | Loss DDPM: 0.37450 | Loss Gap: 0.25034 | Loss Cls: 1.55118
[eval] Epoch 13 — acc: 0.2867, macro_f1: 0.2782
[checkpoint] ¡Nueva mejor accuracy: 0.2867! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF/best.pt

[train] ===== Época 14/100 =====
[train] acc: 0.342 | Loss DDPM: 0.38866 | Loss Gap: 0.25700 | Loss Cls: 1.56554
[eval] Epoch 14 — acc: 0.2413, macro_f1: 0.2216

[train] ===== Época 15/100 =====
[train] acc: 0.356 | Loss DDPM: 0.37332 | Loss Gap: 0.24675 | Loss Cls: 1.56008
[eval] Epoch 15 — acc: 0.2773, macro_f1: 0.2612

[train] ===== Época 16/100 =====
[train] acc: 0.362 | Loss DDPM: 0.35563 | Loss Gap: 0.23605 | Loss Cls: 1.53001
[eval] Epoch 16 — acc: 0.2813, macro_f1: 0.2705

[train] ===== Época 17/100 =====
[train] acc: 0.365 | Loss DDPM: 0.35165 | Loss Gap: 0.23284 | Loss Cls: 1.49934
[eval] Epoch 17 — acc: 0.2987, macro_f1: 0.2917
[checkpoint] ¡Nueva mejor accuracy: 0.2987! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF/best.pt

[train] ===== Época 18/100 =====
[train] acc: 0.398 | Loss DDPM: 0.35222 | Loss Gap: 0.23355 | Loss Cls: 1.49993
[eval] Epoch 18 — acc: 0.2880, macro_f1: 0.2670

[train] ===== Época 19/100 =====
[train] acc: 0.400 | Loss DDPM: 0.36890 | Loss Gap: 0.24212 | Loss Cls: 1.51999
[eval] Epoch 19 — acc: 0.2987, macro_f1: 0.2924

[train] ===== Época 20/100 =====
[train] acc: 0.391 | Loss DDPM: 0.37846 | Loss Gap: 0.24772 | Loss Cls: 1.52655
[eval] Epoch 20 — acc: 0.2413, macro_f1: 0.1931

[train] ===== Época 21/100 =====
[train] acc: 0.415 | Loss DDPM: 0.35087 | Loss Gap: 0.23146 | Loss Cls: 1.49579
[eval] Epoch 21 — acc: 0.2973, macro_f1: 0.2907

[train] ===== Época 22/100 =====
[train] acc: 0.424 | Loss DDPM: 0.34579 | Loss Gap: 0.22904 | Loss Cls: 1.45353
[eval] Epoch 22 — acc: 0.3320, macro_f1: 0.3327
[checkpoint] ¡Nueva mejor accuracy: 0.3320! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF/best.pt

[train] ===== Época 23/100 =====
[train] acc: 0.443 | Loss DDPM: 0.34471 | Loss Gap: 0.22804 | Loss Cls: 1.41347
[eval] Epoch 23 — acc: 0.3120, macro_f1: 0.3096

[train] ===== Época 24/100 =====
[train] acc: 0.469 | Loss DDPM: 0.34819 | Loss Gap: 0.22976 | Loss Cls: 1.43652
[eval] Epoch 24 — acc: 0.2947, macro_f1: 0.2854

[train] ===== Época 25/100 =====
[train] acc: 0.462 | Loss DDPM: 0.36708 | Loss Gap: 0.23907 | Loss Cls: 1.47543
[eval] Epoch 25 — acc: 0.2907, macro_f1: 0.2690

[train] ===== Época 26/100 =====
[train] acc: 0.466 | Loss DDPM: 0.35769 | Loss Gap: 0.23434 | Loss Cls: 1.44921
[eval] Epoch 26 — acc: 0.2867, macro_f1: 0.2602

[train] ===== Época 27/100 =====
[train] acc: 0.505 | Loss DDPM: 0.34174 | Loss Gap: 0.22575 | Loss Cls: 1.39027
[eval] Epoch 27 — acc: 0.3267, macro_f1: 0.3063

[train] ===== Época 28/100 =====
[train] acc: 0.505 | Loss DDPM: 0.33612 | Loss Gap: 0.22154 | Loss Cls: 1.31425
[eval] Epoch 28 — acc: 0.3187, macro_f1: 0.3044

[train] ===== Época 29/100 =====
[train] acc: 0.533 | Loss DDPM: 0.34136 | Loss Gap: 0.22615 | Loss Cls: 1.30368
[eval] Epoch 29 — acc: 0.3027, macro_f1: 0.2720

[train] ===== Época 30/100 =====
[train] acc: 0.533 | Loss DDPM: 0.35207 | Loss Gap: 0.23028 | Loss Cls: 1.37121
[eval] Epoch 30 — acc: 0.3187, macro_f1: 0.2939

[train] ===== Época 31/100 =====
[train] acc: 0.545 | Loss DDPM: 0.36309 | Loss Gap: 0.23490 | Loss Cls: 1.39373
[eval] Epoch 31 — acc: 0.3227, macro_f1: 0.3155

[train] ===== Época 32/100 =====
[train] acc: 0.559 | Loss DDPM: 0.34260 | Loss Gap: 0.22536 | Loss Cls: 1.33399
[eval] Epoch 32 — acc: 0.3413, macro_f1: 0.3355
[checkpoint] ¡Nueva mejor accuracy: 0.3413! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF/best.pt

[train] ===== Época 33/100 =====
[train] acc: 0.586 | Loss DDPM: 0.33163 | Loss Gap: 0.21865 | Loss Cls: 1.23060
[eval] Epoch 33 — acc: 0.3293, macro_f1: 0.3144

[train] ===== Época 34/100 =====
[train] acc: 0.598 | Loss DDPM: 0.33116 | Loss Gap: 0.21855 | Loss Cls: 1.14999
[eval] Epoch 34 — acc: 0.3467, macro_f1: 0.3397
[checkpoint] ¡Nueva mejor accuracy: 0.3467! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF/best.pt

[train] ===== Época 35/100 =====
[train] acc: 0.619 | Loss DDPM: 0.33347 | Loss Gap: 0.22031 | Loss Cls: 1.21148
[eval] Epoch 35 — acc: 0.2907, macro_f1: 0.2578

[train] ===== Época 36/100 =====
[train] acc: 0.599 | Loss DDPM: 0.35838 | Loss Gap: 0.23227 | Loss Cls: 1.27820
[eval] Epoch 36 — acc: 0.3240, macro_f1: 0.3212

[train] ===== Época 37/100 =====
[train] acc: 0.602 | Loss DDPM: 0.35209 | Loss Gap: 0.22907 | Loss Cls: 1.28643
[eval] Epoch 37 — acc: 0.3400, macro_f1: 0.3376

[train] ===== Época 38/100 =====
[train] acc: 0.654 | Loss DDPM: 0.33503 | Loss Gap: 0.22016 | Loss Cls: 1.17010
[eval] Epoch 38 — acc: 0.3587, macro_f1: 0.3534
[checkpoint] ¡Nueva mejor accuracy: 0.3587! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF/best.pt

[train] ===== Época 39/100 =====
[train] acc: 0.664 | Loss DDPM: 0.33116 | Loss Gap: 0.21759 | Loss Cls: 1.06295
[eval] Epoch 39 — acc: 0.3760, macro_f1: 0.3739
[checkpoint] ¡Nueva mejor accuracy: 0.3760! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF/best.pt

[train] ===== Época 40/100 =====
[train] acc: 0.693 | Loss DDPM: 0.32792 | Loss Gap: 0.21499 | Loss Cls: 1.03447
[eval] Epoch 40 — acc: 0.3493, macro_f1: 0.3370

[train] ===== Época 41/100 =====
[train] acc: 0.680 | Loss DDPM: 0.34407 | Loss Gap: 0.22364 | Loss Cls: 1.14381
[eval] Epoch 41 — acc: 0.3613, macro_f1: 0.3519

[train] ===== Época 42/100 =====
[train] acc: 0.661 | Loss DDPM: 0.35637 | Loss Gap: 0.22898 | Loss Cls: 1.19288
[eval] Epoch 42 — acc: 0.3613, macro_f1: 0.3435

[train] ===== Época 43/100 =====
[train] acc: 0.680 | Loss DDPM: 0.33685 | Loss Gap: 0.21897 | Loss Cls: 1.10725
[eval] Epoch 43 — acc: 0.3600, macro_f1: 0.3499

[train] ===== Época 44/100 =====
[train] acc: 0.721 | Loss DDPM: 0.32873 | Loss Gap: 0.21409 | Loss Cls: 0.98940
[eval] Epoch 44 — acc: 0.3813, macro_f1: 0.3797
[checkpoint] ¡Nueva mejor accuracy: 0.3813! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF/best.pt

[train] ===== Época 45/100 =====
[train] acc: 0.733 | Loss DDPM: 0.32204 | Loss Gap: 0.20914 | Loss Cls: 0.87380
[eval] Epoch 45 — acc: 0.3707, macro_f1: 0.3677

[train] ===== Época 46/100 =====
[train] acc: 0.749 | Loss DDPM: 0.32705 | Loss Gap: 0.21275 | Loss Cls: 0.92816
[eval] Epoch 46 — acc: 0.3600, macro_f1: 0.3480

[train] ===== Época 47/100 =====
[train] acc: 0.708 | Loss DDPM: 0.34850 | Loss Gap: 0.22348 | Loss Cls: 1.08283
[eval] Epoch 47 — acc: 0.3427, macro_f1: 0.3382

[train] ===== Época 48/100 =====
[train] acc: 0.727 | Loss DDPM: 0.34588 | Loss Gap: 0.22179 | Loss Cls: 1.06408
[eval] Epoch 48 — acc: 0.3587, macro_f1: 0.3574

[train] ===== Época 49/100 =====
[train] acc: 0.761 | Loss DDPM: 0.32643 | Loss Gap: 0.21164 | Loss Cls: 0.94362
[eval] Epoch 49 — acc: 0.3880, macro_f1: 0.3852
[checkpoint] ¡Nueva mejor accuracy: 0.3880! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF/best.pt

[train] ===== Época 50/100 =====
[train] acc: 0.775 | Loss DDPM: 0.32557 | Loss Gap: 0.21076 | Loss Cls: 0.80040
[eval] Epoch 50 — acc: 0.3973, macro_f1: 0.3965
[checkpoint] ¡Nueva mejor accuracy: 0.3973! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF/best.pt

[train] ===== Época 51/100 =====
[train] acc: 0.804 | Loss DDPM: 0.32418 | Loss Gap: 0.20974 | Loss Cls: 0.74469
[eval] Epoch 51 — acc: 0.3573, macro_f1: 0.3535

[train] ===== Época 52/100 =====
[train] acc: 0.799 | Loss DDPM: 0.33033 | Loss Gap: 0.21259 | Loss Cls: 0.88223
[eval] Epoch 52 — acc: 0.3360, macro_f1: 0.3188

[train] ===== Época 53/100 =====
[train] acc: 0.742 | Loss DDPM: 0.34924 | Loss Gap: 0.22222 | Loss Cls: 1.02583
[eval] Epoch 53 — acc: 0.3547, macro_f1: 0.3513

[train] ===== Época 54/100 =====
[train] acc: 0.784 | Loss DDPM: 0.33607 | Loss Gap: 0.21600 | Loss Cls: 0.90666
[eval] Epoch 54 — acc: 0.3573, macro_f1: 0.3441

[train] ===== Época 55/100 =====
[train] acc: 0.826 | Loss DDPM: 0.32360 | Loss Gap: 0.20836 | Loss Cls: 0.75758
[eval] Epoch 55 — acc: 0.3867, macro_f1: 0.3873

[train] ===== Época 56/100 =====
[train] acc: 0.829 | Loss DDPM: 0.32287 | Loss Gap: 0.20729 | Loss Cls: 0.64123
[eval] Epoch 56 — acc: 0.3907, macro_f1: 0.3921

[train] ===== Época 57/100 =====
[train] acc: 0.857 | Loss DDPM: 0.32191 | Loss Gap: 0.20709 | Loss Cls: 0.64817
[eval] Epoch 57 — acc: 0.4053, macro_f1: 0.3973
[checkpoint] ¡Nueva mejor accuracy: 0.4053! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF/best.pt

[train] ===== Época 58/100 =====
[train] acc: 0.798 | Loss DDPM: 0.33794 | Loss Gap: 0.21488 | Loss Cls: 0.88418
[eval] Epoch 58 — acc: 0.3573, macro_f1: 0.3403

[train] ===== Época 59/100 =====
[train] acc: 0.791 | Loss DDPM: 0.34882 | Loss Gap: 0.22171 | Loss Cls: 0.92497
[eval] Epoch 59 — acc: 0.3640, macro_f1: 0.3627

[train] ===== Época 60/100 =====
[train] acc: 0.848 | Loss DDPM: 0.32456 | Loss Gap: 0.20800 | Loss Cls: 0.73344
[eval] Epoch 60 — acc: 0.3800, macro_f1: 0.3761

[train] ===== Época 61/100 =====
[train] acc: 0.862 | Loss DDPM: 0.31719 | Loss Gap: 0.20324 | Loss Cls: 0.57598
[eval] Epoch 61 — acc: 0.4267, macro_f1: 0.4264
[checkpoint] ¡Nueva mejor accuracy: 0.4267! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF/best.pt

[train] ===== Época 62/100 =====
[train] acc: 0.887 | Loss DDPM: 0.31942 | Loss Gap: 0.20410 | Loss Cls: 0.50755
[eval] Epoch 62 — acc: 0.4053, macro_f1: 0.4050

[train] ===== Época 63/100 =====
[train] acc: 0.867 | Loss DDPM: 0.32107 | Loss Gap: 0.20554 | Loss Cls: 0.64865
[eval] Epoch 63 — acc: 0.3867, macro_f1: 0.3812

[train] ===== Época 64/100 =====
[train] acc: 0.798 | Loss DDPM: 0.34575 | Loss Gap: 0.21906 | Loss Cls: 0.90254
[eval] Epoch 64 — acc: 0.3667, macro_f1: 0.3520

[train] ===== Época 65/100 =====
[train] acc: 0.855 | Loss DDPM: 0.32983 | Loss Gap: 0.21011 | Loss Cls: 0.71387
[eval] Epoch 65 — acc: 0.3893, macro_f1: 0.3768

[train] ===== Época 66/100 =====
[train] acc: 0.904 | Loss DDPM: 0.31781 | Loss Gap: 0.20382 | Loss Cls: 0.54219
[eval] Epoch 66 — acc: 0.3933, macro_f1: 0.3888

[train] ===== Época 67/100 =====
[train] acc: 0.908 | Loss DDPM: 0.31586 | Loss Gap: 0.20193 | Loss Cls: 0.41325
[eval] Epoch 67 — acc: 0.4147, macro_f1: 0.4122

[train] ===== Época 68/100 =====
[train] acc: 0.919 | Loss DDPM: 0.31951 | Loss Gap: 0.20509 | Loss Cls: 0.43856
[eval] Epoch 68 — acc: 0.4027, macro_f1: 0.4004

[train] ===== Época 69/100 =====
[train] acc: 0.856 | Loss DDPM: 0.33176 | Loss Gap: 0.21131 | Loss Cls: 0.65546
[eval] Epoch 69 — acc: 0.3653, macro_f1: 0.3655

[train] ===== Época 70/100 =====
[train] acc: 0.825 | Loss DDPM: 0.33884 | Loss Gap: 0.21489 | Loss Cls: 0.78481
[eval] Epoch 70 — acc: 0.3613, macro_f1: 0.3464

[train] ===== Época 71/100 =====
[train] acc: 0.914 | Loss DDPM: 0.32103 | Loss Gap: 0.20543 | Loss Cls: 0.55774
[eval] Epoch 71 — acc: 0.4187, macro_f1: 0.4169

[train] ===== Época 72/100 =====
[train] acc: 0.935 | Loss DDPM: 0.31727 | Loss Gap: 0.20346 | Loss Cls: 0.38823
[eval] Epoch 72 — acc: 0.4200, macro_f1: 0.4190

[train] ===== Época 73/100 =====
[train] acc: 0.947 | Loss DDPM: 0.31325 | Loss Gap: 0.20047 | Loss Cls: 0.30675
[eval] Epoch 73 — acc: 0.4133, macro_f1: 0.4124

[train] ===== Época 74/100 =====
[train] acc: 0.931 | Loss DDPM: 0.31664 | Loss Gap: 0.20309 | Loss Cls: 0.39529
[eval] Epoch 74 — acc: 0.3720, macro_f1: 0.3616

[train] ===== Época 75/100 =====
[train] acc: 0.835 | Loss DDPM: 0.33430 | Loss Gap: 0.21197 | Loss Cls: 0.75837
[eval] Epoch 75 — acc: 0.3827, macro_f1: 0.3729

[train] ===== Época 76/100 =====
[train] acc: 0.893 | Loss DDPM: 0.33440 | Loss Gap: 0.21324 | Loss Cls: 0.62858
[eval] Epoch 76 — acc: 0.4040, macro_f1: 0.3960

[train] ===== Época 77/100 =====
[train] acc: 0.940 | Loss DDPM: 0.31673 | Loss Gap: 0.20282 | Loss Cls: 0.40013
[eval] Epoch 77 — acc: 0.4147, macro_f1: 0.4145

[train] ===== Época 78/100 =====
[train] acc: 0.951 | Loss DDPM: 0.31360 | Loss Gap: 0.20056 | Loss Cls: 0.27375
[eval] Epoch 78 — acc: 0.4427, macro_f1: 0.4423
[checkpoint] ¡Nueva mejor accuracy: 0.4427! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF/best.pt

[train] ===== Época 79/100 =====
[train] acc: 0.965 | Loss DDPM: 0.31201 | Loss Gap: 0.19955 | Loss Cls: 0.24556
[eval] Epoch 79 — acc: 0.4173, macro_f1: 0.4167

[train] ===== Época 80/100 =====
[train] acc: 0.918 | Loss DDPM: 0.32586 | Loss Gap: 0.20757 | Loss Cls: 0.42714
[eval] Epoch 80 — acc: 0.3840, macro_f1: 0.3827

[train] ===== Época 81/100 =====
[train] acc: 0.835 | Loss DDPM: 0.33541 | Loss Gap: 0.21253 | Loss Cls: 0.79935
[eval] Epoch 81 — acc: 0.3507, macro_f1: 0.3357

[train] ===== Época 82/100 =====
[train] acc: 0.932 | Loss DDPM: 0.31708 | Loss Gap: 0.20292 | Loss Cls: 0.46258
[eval] Epoch 82 — acc: 0.4013, macro_f1: 0.3994

[train] ===== Época 83/100 =====
[train] acc: 0.959 | Loss DDPM: 0.31350 | Loss Gap: 0.20118 | Loss Cls: 0.29123
[eval] Epoch 83 — acc: 0.4147, macro_f1: 0.4145

[train] ===== Época 84/100 =====
[train] acc: 0.964 | Loss DDPM: 0.31261 | Loss Gap: 0.19979 | Loss Cls: 0.21462
[eval] Epoch 84 — acc: 0.4280, macro_f1: 0.4279

[train] ===== Época 85/100 =====
[train] acc: 0.967 | Loss DDPM: 0.31561 | Loss Gap: 0.20239 | Loss Cls: 0.23194
[eval] Epoch 85 — acc: 0.4000, macro_f1: 0.3911

[train] ===== Época 86/100 =====
[train] acc: 0.862 | Loss DDPM: 0.33128 | Loss Gap: 0.21056 | Loss Cls: 0.63208
[eval] Epoch 86 — acc: 0.3427, macro_f1: 0.3247

[train] ===== Época 87/100 =====
[train] acc: 0.890 | Loss DDPM: 0.33219 | Loss Gap: 0.21186 | Loss Cls: 0.62042
[eval] Epoch 87 — acc: 0.4053, macro_f1: 0.4044

[train] ===== Época 88/100 =====
[train] acc: 0.968 | Loss DDPM: 0.31731 | Loss Gap: 0.20329 | Loss Cls: 0.29222
[eval] Epoch 88 — acc: 0.4213, macro_f1: 0.4208

[train] ===== Época 89/100 =====
[train] acc: 0.976 | Loss DDPM: 0.30851 | Loss Gap: 0.19725 | Loss Cls: 0.19042
[eval] Epoch 89 — acc: 0.4253, macro_f1: 0.4240

[train] ===== Época 90/100 =====
[train] acc: 0.981 | Loss DDPM: 0.31392 | Loss Gap: 0.20119 | Loss Cls: 0.17081
[eval] Epoch 90 — acc: 0.4173, macro_f1: 0.4137

[train] ===== Época 91/100 =====
[train] acc: 0.951 | Loss DDPM: 0.32000 | Loss Gap: 0.20447 | Loss Cls: 0.26790
[eval] Epoch 91 — acc: 0.3573, macro_f1: 0.3544

[train] ===== Época 92/100 =====
[train] acc: 0.857 | Loss DDPM: 0.33362 | Loss Gap: 0.21161 | Loss Cls: 0.76867
[eval] Epoch 92 — acc: 0.3760, macro_f1: 0.3682

[train] ===== Época 93/100 =====
[train] acc: 0.952 | Loss DDPM: 0.31777 | Loss Gap: 0.20322 | Loss Cls: 0.37551
[eval] Epoch 93 — acc: 0.4280, macro_f1: 0.4285

[train] ===== Época 94/100 =====
[train] acc: 0.978 | Loss DDPM: 0.31192 | Loss Gap: 0.19993 | Loss Cls: 0.21059
[eval] Epoch 94 — acc: 0.4333, macro_f1: 0.4336

[train] ===== Época 95/100 =====
[train] acc: 0.981 | Loss DDPM: 0.30941 | Loss Gap: 0.19746 | Loss Cls: 0.15177
[eval] Epoch 95 — acc: 0.4280, macro_f1: 0.4273

[train] ===== Época 96/100 =====
[train] acc: 0.986 | Loss DDPM: 0.31071 | Loss Gap: 0.19933 | Loss Cls: 0.15249
[eval] Epoch 96 — acc: 0.4187, macro_f1: 0.4150

[train] ===== Época 97/100 =====
[train] acc: 0.882 | Loss DDPM: 0.32679 | Loss Gap: 0.20757 | Loss Cls: 0.56039
[eval] Epoch 97 — acc: 0.3240, macro_f1: 0.2831

[train] ===== Época 98/100 =====
[train] acc: 0.928 | Loss DDPM: 0.32869 | Loss Gap: 0.20900 | Loss Cls: 0.52826
[eval] Epoch 98 — acc: 0.4187, macro_f1: 0.4163

[train] ===== Época 99/100 =====
[train] acc: 0.975 | Loss DDPM: 0.31299 | Loss Gap: 0.20082 | Loss Cls: 0.25394
[eval] Epoch 99 — acc: 0.4587, macro_f1: 0.4578
[checkpoint] ¡Nueva mejor accuracy: 0.4587! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF/best.pt

[train] ===== Época 100/100 =====
[train] acc: 0.984 | Loss DDPM: 0.30989 | Loss Gap: 0.19815 | Loss Cls: 0.14635
[eval] Epoch 100 — acc: 0.4533, macro_f1: 0.4529

[test] Evaluando el mejor modelo en el conjunto de test…
[test] Checkpoint restaurado desde /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/models/DiffE_TF/best.pt
[test] Reporte completo:

  metrics:
            accuracy: 0.4213
    balanced_accuracy: 0.4213
            macro_f1: 0.4214
     macro_precision: 0.4218
        macro_recall: 0.4213
         roc_auc_ovo: 0.7242
                 mcc: 0.2767
         cohen_kappa: 0.2767
    confusion_matrix: [[63 25 21 23 18]
 [14 69 25 18 24]
 [17 29 59 25 20]
 [23 15 24 63 25]
 [22 19 23 24 62]]

  baseline_random:
            accuracy: 0.2000
    balanced_accuracy: 0.2011
            macro_f1: 0.2010
     macro_precision: 0.2013
        macro_recall: 0.2011
         roc_auc_ovo: 0.5000
                 mcc: 0.0014
         cohen_kappa: 0.0014

  improvement_%:
            accuracy: 110.6667
    balanced_accuracy: 109.4657
            macro_f1: 109.6583
     macro_precision: 109.5068
        macro_recall: 109.4657
         roc_auc_ovo: 44.8391
                 mcc: 19191.8889
         cohen_kappa: 19202.3256

  confusion_matrix:
[[63 25 21 23 18]
 [14 69 25 18 24]
 [17 29 59 25 20]
 [23 15 24 63 25]
 [22 19 23 24 62]]
