[config] Parámetros: Namespace(dataset_file='/media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/data/processed/BCI2020/BCI_raw.npz', dataset='BCI', device='cuda:0', num_epochs=100, batch_train=250, batch_eval=32, seed=42, alpha=10.0, subject_id=None, num_classes=5, channels=64, n_T=100, ddpm_dim=128, encoder_dim=64, fc_dim=64, exp_dir='/media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL')
[setup] Semilla fijada a 42
[setup] Usando dispositivo: cuda:0
[data] Cargando datos del dataset BCI
[INFO] Cargando datos desde: /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/data/processed/BCI2020/BCI_raw.npz
[INFO] Aplicando z-score normalization por trial
    - Forma de entrada (esperada): (trials, canales, muestras) = torch.Size([4500, 64, 795])
    - Calculando media y desviación estándar por trial
    - Mean shape: torch.Size([4500, 1, 1]), Std shape: torch.Size([4500, 1, 1])
    - Normalización completada. Forma de salida: torch.Size([4500, 64, 795])
[INFO] Padding aplicado: se añaden 5 valores a la derecha (dim original: 795)
[INFO] Dimensión temporal final: 800
[INFO] Aplicando z-score normalization por trial
    - Forma de entrada (esperada): (trials, canales, muestras) = torch.Size([750, 64, 795])
    - Calculando media y desviación estándar por trial
    - Mean shape: torch.Size([750, 1, 1]), Std shape: torch.Size([750, 1, 1])
    - Normalización completada. Forma de salida: torch.Size([750, 64, 795])
[INFO] Padding aplicado: se añaden 5 valores a la derecha (dim original: 795)
[INFO] Dimensión temporal final: 800
[INFO] Aplicando z-score normalization por trial
    - Forma de entrada (esperada): (trials, canales, muestras) = torch.Size([750, 64, 795])
    - Calculando media y desviación estándar por trial
    - Mean shape: torch.Size([750, 1, 1]), Std shape: torch.Size([750, 1, 1])
    - Normalización completada. Forma de salida: torch.Size([750, 64, 795])
[INFO] Padding aplicado: se añaden 5 valores a la derecha (dim original: 795)
[INFO] Dimensión temporal final: 800
[INFO] Cargando datos de todos los sujetos (concatenados)
[INFO] Dimensiones finales:
    - X_train: torch.Size([4500, 64, 800]), y_train: torch.Size([4500])
    - X_val:   torch.Size([750, 64, 800]), y_val:   torch.Size([750])
    - X_test:  torch.Size([750, 64, 800]), y_test:  torch.Size([750])
[INFO] Creando DataLoaders:
    - Batch size (train): 250  | Shuffle: True
    - Batch size (eval) : 32  | Shuffle: False
[INFO] Número de batches:
    - Train: 18 batches
    - Val  : 24 batches
    - Test : 24 batches
[model] Inicializando modelos...
[model] FLOPs: 0.01 GFLOPs | Peso: 0.70 MB | Parámetros: 0.18 M
[setup] Configurando optimizadores y schedulers: base_lr=0.0003, max_lr=0.003, step_size=50
[setup] EMA configurada en el clasificatorio (beta=0.95)
[train] Inicio del bucle de entrenamiento por 100 épocas

[train] ===== Época 1/100 =====
[train] acc: 0.239 | Loss DDPM: 0.62689 | Loss Gap: 0.47672 | Loss Cls: 1.68688
[eval] Epoch 1 — acc: 0.2253, macro_f1: 0.2146
[checkpoint] ¡Nueva mejor accuracy: 0.2253! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 2/100 =====
[train] acc: 0.273 | Loss DDPM: 0.50002 | Loss Gap: 0.39905 | Loss Cls: 1.63480
[eval] Epoch 2 — acc: 0.2560, macro_f1: 0.2425
[checkpoint] ¡Nueva mejor accuracy: 0.2560! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 3/100 =====
[train] acc: 0.297 | Loss DDPM: 0.46330 | Loss Gap: 0.38733 | Loss Cls: 1.60018
[eval] Epoch 3 — acc: 0.2760, macro_f1: 0.2615
[checkpoint] ¡Nueva mejor accuracy: 0.2760! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 4/100 =====
[train] acc: 0.329 | Loss DDPM: 0.42287 | Loss Gap: 0.30364 | Loss Cls: 1.57623
[eval] Epoch 4 — acc: 0.2867, macro_f1: 0.2791
[checkpoint] ¡Nueva mejor accuracy: 0.2867! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 5/100 =====
[train] acc: 0.340 | Loss DDPM: 0.40024 | Loss Gap: 0.25906 | Loss Cls: 1.54607
[eval] Epoch 5 — acc: 0.3027, macro_f1: 0.2975
[checkpoint] ¡Nueva mejor accuracy: 0.3027! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 6/100 =====
[train] acc: 0.334 | Loss DDPM: 0.38629 | Loss Gap: 0.24507 | Loss Cls: 1.51100
[eval] Epoch 6 — acc: 0.3133, macro_f1: 0.3082
[checkpoint] ¡Nueva mejor accuracy: 0.3133! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 7/100 =====
[train] acc: 0.375 | Loss DDPM: 0.38958 | Loss Gap: 0.25128 | Loss Cls: 1.50957
[eval] Epoch 7 — acc: 0.3187, macro_f1: 0.3130
[checkpoint] ¡Nueva mejor accuracy: 0.3187! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 8/100 =====
[train] acc: 0.418 | Loss DDPM: 0.40900 | Loss Gap: 0.29593 | Loss Cls: 1.50853
[eval] Epoch 8 — acc: 0.3307, macro_f1: 0.3242
[checkpoint] ¡Nueva mejor accuracy: 0.3307! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 9/100 =====
[train] acc: 0.444 | Loss DDPM: 0.40198 | Loss Gap: 0.29753 | Loss Cls: 1.49005
[eval] Epoch 9 — acc: 0.3333, macro_f1: 0.3220
[checkpoint] ¡Nueva mejor accuracy: 0.3333! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 10/100 =====
[train] acc: 0.469 | Loss DDPM: 0.37727 | Loss Gap: 0.24438 | Loss Cls: 1.43338
[eval] Epoch 10 — acc: 0.3440, macro_f1: 0.3414
[checkpoint] ¡Nueva mejor accuracy: 0.3440! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 11/100 =====
[train] acc: 0.485 | Loss DDPM: 0.36816 | Loss Gap: 0.22934 | Loss Cls: 1.35743
[eval] Epoch 11 — acc: 0.3627, macro_f1: 0.3589
[checkpoint] ¡Nueva mejor accuracy: 0.3627! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 12/100 =====
[train] acc: 0.498 | Loss DDPM: 0.36536 | Loss Gap: 0.22676 | Loss Cls: 1.30777
[eval] Epoch 12 — acc: 0.3587, macro_f1: 0.3537

[train] ===== Época 13/100 =====
[train] acc: 0.544 | Loss DDPM: 0.37453 | Loss Gap: 0.24261 | Loss Cls: 1.31519
[eval] Epoch 13 — acc: 0.3787, macro_f1: 0.3731
[checkpoint] ¡Nueva mejor accuracy: 0.3787! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 14/100 =====
[train] acc: 0.561 | Loss DDPM: 0.39041 | Loss Gap: 0.29264 | Loss Cls: 1.32840
[eval] Epoch 14 — acc: 0.3693, macro_f1: 0.3628

[train] ===== Época 15/100 =====
[train] acc: 0.598 | Loss DDPM: 0.37084 | Loss Gap: 0.25315 | Loss Cls: 1.28161
[eval] Epoch 15 — acc: 0.3987, macro_f1: 0.3913
[checkpoint] ¡Nueva mejor accuracy: 0.3987! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 16/100 =====
[train] acc: 0.619 | Loss DDPM: 0.35625 | Loss Gap: 0.22302 | Loss Cls: 1.16380
[eval] Epoch 16 — acc: 0.4120, macro_f1: 0.4093
[checkpoint] ¡Nueva mejor accuracy: 0.4120! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 17/100 =====
[train] acc: 0.608 | Loss DDPM: 0.35190 | Loss Gap: 0.21738 | Loss Cls: 1.08036
[eval] Epoch 17 — acc: 0.4253, macro_f1: 0.4222
[checkpoint] ¡Nueva mejor accuracy: 0.4253! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 18/100 =====
[train] acc: 0.651 | Loss DDPM: 0.34883 | Loss Gap: 0.21869 | Loss Cls: 1.07170
[eval] Epoch 18 — acc: 0.4160, macro_f1: 0.4104

[train] ===== Época 19/100 =====
[train] acc: 0.685 | Loss DDPM: 0.37311 | Loss Gap: 0.25210 | Loss Cls: 1.11666
[eval] Epoch 19 — acc: 0.4080, macro_f1: 0.4072

[train] ===== Época 20/100 =====
[train] acc: 0.696 | Loss DDPM: 0.37660 | Loss Gap: 0.27938 | Loss Cls: 1.12686
[eval] Epoch 20 — acc: 0.4360, macro_f1: 0.4354
[checkpoint] ¡Nueva mejor accuracy: 0.4360! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 21/100 =====
[train] acc: 0.715 | Loss DDPM: 0.35421 | Loss Gap: 0.22773 | Loss Cls: 1.02365
[eval] Epoch 21 — acc: 0.4440, macro_f1: 0.4430
[checkpoint] ¡Nueva mejor accuracy: 0.4440! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 22/100 =====
[train] acc: 0.718 | Loss DDPM: 0.34347 | Loss Gap: 0.21250 | Loss Cls: 0.91782
[eval] Epoch 22 — acc: 0.4453, macro_f1: 0.4438
[checkpoint] ¡Nueva mejor accuracy: 0.4453! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 23/100 =====
[train] acc: 0.725 | Loss DDPM: 0.33709 | Loss Gap: 0.20784 | Loss Cls: 0.85388
[eval] Epoch 23 — acc: 0.4573, macro_f1: 0.4564
[checkpoint] ¡Nueva mejor accuracy: 0.4573! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 24/100 =====
[train] acc: 0.753 | Loss DDPM: 0.34687 | Loss Gap: 0.22029 | Loss Cls: 0.88347
[eval] Epoch 24 — acc: 0.4427, macro_f1: 0.4397

[train] ===== Época 25/100 =====
[train] acc: 0.765 | Loss DDPM: 0.36971 | Loss Gap: 0.26619 | Loss Cls: 0.96801
[eval] Epoch 25 — acc: 0.4427, macro_f1: 0.4404

[train] ===== Época 26/100 =====
[train] acc: 0.775 | Loss DDPM: 0.35654 | Loss Gap: 0.25018 | Loss Cls: 0.90465
[eval] Epoch 26 — acc: 0.4653, macro_f1: 0.4629
[checkpoint] ¡Nueva mejor accuracy: 0.4653! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 27/100 =====
[train] acc: 0.790 | Loss DDPM: 0.33929 | Loss Gap: 0.21210 | Loss Cls: 0.81431
[eval] Epoch 27 — acc: 0.4707, macro_f1: 0.4699
[checkpoint] ¡Nueva mejor accuracy: 0.4707! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 28/100 =====
[train] acc: 0.776 | Loss DDPM: 0.33724 | Loss Gap: 0.20785 | Loss Cls: 0.70634
[eval] Epoch 28 — acc: 0.4707, macro_f1: 0.4706

[train] ===== Época 29/100 =====
[train] acc: 0.806 | Loss DDPM: 0.33576 | Loss Gap: 0.20819 | Loss Cls: 0.68841
[eval] Epoch 29 — acc: 0.4773, macro_f1: 0.4771
[checkpoint] ¡Nueva mejor accuracy: 0.4773! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 30/100 =====
[train] acc: 0.812 | Loss DDPM: 0.35441 | Loss Gap: 0.23195 | Loss Cls: 0.75557
[eval] Epoch 30 — acc: 0.4587, macro_f1: 0.4585

[train] ===== Época 31/100 =====
[train] acc: 0.822 | Loss DDPM: 0.36508 | Loss Gap: 0.27261 | Loss Cls: 0.83294
[eval] Epoch 31 — acc: 0.4827, macro_f1: 0.4834
[checkpoint] ¡Nueva mejor accuracy: 0.4827! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 32/100 =====
[train] acc: 0.839 | Loss DDPM: 0.34030 | Loss Gap: 0.22003 | Loss Cls: 0.71967
[eval] Epoch 32 — acc: 0.5040, macro_f1: 0.5044
[checkpoint] ¡Nueva mejor accuracy: 0.5040! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 33/100 =====
[train] acc: 0.839 | Loss DDPM: 0.33503 | Loss Gap: 0.20747 | Loss Cls: 0.63324
[eval] Epoch 33 — acc: 0.5120, macro_f1: 0.5123
[checkpoint] ¡Nueva mejor accuracy: 0.5120! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 34/100 =====
[train] acc: 0.833 | Loss DDPM: 0.33181 | Loss Gap: 0.20430 | Loss Cls: 0.54815
[eval] Epoch 34 — acc: 0.5120, macro_f1: 0.5123

[train] ===== Época 35/100 =====
[train] acc: 0.860 | Loss DDPM: 0.33531 | Loss Gap: 0.20959 | Loss Cls: 0.59219
[eval] Epoch 35 — acc: 0.5067, macro_f1: 0.5076

[train] ===== Época 36/100 =====
[train] acc: 0.859 | Loss DDPM: 0.35465 | Loss Gap: 0.25287 | Loss Cls: 0.70140
[eval] Epoch 36 — acc: 0.5040, macro_f1: 0.5042

[train] ===== Época 37/100 =====
[train] acc: 0.864 | Loss DDPM: 0.35255 | Loss Gap: 0.24823 | Loss Cls: 0.68349
[eval] Epoch 37 — acc: 0.4947, macro_f1: 0.4949

[train] ===== Época 38/100 =====
[train] acc: 0.873 | Loss DDPM: 0.33083 | Loss Gap: 0.20784 | Loss Cls: 0.57127
[eval] Epoch 38 — acc: 0.5213, macro_f1: 0.5215
[checkpoint] ¡Nueva mejor accuracy: 0.5213! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 39/100 =====
[train] acc: 0.868 | Loss DDPM: 0.32558 | Loss Gap: 0.20139 | Loss Cls: 0.48419
[eval] Epoch 39 — acc: 0.5200, macro_f1: 0.5197

[train] ===== Época 40/100 =====
[train] acc: 0.886 | Loss DDPM: 0.32647 | Loss Gap: 0.20208 | Loss Cls: 0.45111
[eval] Epoch 40 — acc: 0.5093, macro_f1: 0.5090

[train] ===== Época 41/100 =====
[train] acc: 0.896 | Loss DDPM: 0.34091 | Loss Gap: 0.22009 | Loss Cls: 0.51346
[eval] Epoch 41 — acc: 0.5280, macro_f1: 0.5278
[checkpoint] ¡Nueva mejor accuracy: 0.5280! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 42/100 =====
[train] acc: 0.878 | Loss DDPM: 0.35667 | Loss Gap: 0.26088 | Loss Cls: 0.64510
[eval] Epoch 42 — acc: 0.4987, macro_f1: 0.4983

[train] ===== Época 43/100 =====
[train] acc: 0.905 | Loss DDPM: 0.33741 | Loss Gap: 0.22307 | Loss Cls: 0.53918
[eval] Epoch 43 — acc: 0.5373, macro_f1: 0.5367
[checkpoint] ¡Nueva mejor accuracy: 0.5373! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 44/100 =====
[train] acc: 0.907 | Loss DDPM: 0.32879 | Loss Gap: 0.20430 | Loss Cls: 0.43325
[eval] Epoch 44 — acc: 0.5467, macro_f1: 0.5465
[checkpoint] ¡Nueva mejor accuracy: 0.5467! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 45/100 =====
[train] acc: 0.907 | Loss DDPM: 0.32424 | Loss Gap: 0.20030 | Loss Cls: 0.37601
[eval] Epoch 45 — acc: 0.5400, macro_f1: 0.5392

[train] ===== Época 46/100 =====
[train] acc: 0.915 | Loss DDPM: 0.32818 | Loss Gap: 0.20602 | Loss Cls: 0.38111
[eval] Epoch 46 — acc: 0.5253, macro_f1: 0.5242

[train] ===== Época 47/100 =====
[train] acc: 0.903 | Loss DDPM: 0.34752 | Loss Gap: 0.23604 | Loss Cls: 0.49522
[eval] Epoch 47 — acc: 0.5320, macro_f1: 0.5326

[train] ===== Época 48/100 =====
[train] acc: 0.906 | Loss DDPM: 0.34445 | Loss Gap: 0.24854 | Loss Cls: 0.55436
[eval] Epoch 48 — acc: 0.5413, macro_f1: 0.5413

[train] ===== Época 49/100 =====
[train] acc: 0.920 | Loss DDPM: 0.32775 | Loss Gap: 0.20699 | Loss Cls: 0.43800
[eval] Epoch 49 — acc: 0.5507, macro_f1: 0.5498
[checkpoint] ¡Nueva mejor accuracy: 0.5507! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 50/100 =====
[train] acc: 0.922 | Loss DDPM: 0.32406 | Loss Gap: 0.20023 | Loss Cls: 0.32797
[eval] Epoch 50 — acc: 0.5627, macro_f1: 0.5620
[checkpoint] ¡Nueva mejor accuracy: 0.5627! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 51/100 =====
[train] acc: 0.932 | Loss DDPM: 0.32065 | Loss Gap: 0.19813 | Loss Cls: 0.29291
[eval] Epoch 51 — acc: 0.5480, macro_f1: 0.5480

[train] ===== Época 52/100 =====
[train] acc: 0.934 | Loss DDPM: 0.32964 | Loss Gap: 0.21314 | Loss Cls: 0.36902
[eval] Epoch 52 — acc: 0.5387, macro_f1: 0.5387

[train] ===== Época 53/100 =====
[train] acc: 0.900 | Loss DDPM: 0.34892 | Loss Gap: 0.25137 | Loss Cls: 0.57205
[eval] Epoch 53 — acc: 0.5213, macro_f1: 0.5214

[train] ===== Época 54/100 =====
[train] acc: 0.927 | Loss DDPM: 0.33381 | Loss Gap: 0.21954 | Loss Cls: 0.43720
[eval] Epoch 54 — acc: 0.5387, macro_f1: 0.5380

[train] ===== Época 55/100 =====
[train] acc: 0.935 | Loss DDPM: 0.32331 | Loss Gap: 0.20121 | Loss Cls: 0.31523
[eval] Epoch 55 — acc: 0.5627, macro_f1: 0.5624

[train] ===== Época 56/100 =====
[train] acc: 0.936 | Loss DDPM: 0.31907 | Loss Gap: 0.19696 | Loss Cls: 0.25913
[eval] Epoch 56 — acc: 0.5533, macro_f1: 0.5533

[train] ===== Época 57/100 =====
[train] acc: 0.943 | Loss DDPM: 0.32023 | Loss Gap: 0.19931 | Loss Cls: 0.27315
[eval] Epoch 57 — acc: 0.5413, macro_f1: 0.5414

[train] ===== Época 58/100 =====
[train] acc: 0.934 | Loss DDPM: 0.33887 | Loss Gap: 0.22937 | Loss Cls: 0.35113
[eval] Epoch 58 — acc: 0.5293, macro_f1: 0.5291

[train] ===== Época 59/100 =====
[train] acc: 0.913 | Loss DDPM: 0.34443 | Loss Gap: 0.24340 | Loss Cls: 0.50193
[eval] Epoch 59 — acc: 0.5413, macro_f1: 0.5418

[train] ===== Época 60/100 =====
[train] acc: 0.940 | Loss DDPM: 0.32477 | Loss Gap: 0.20751 | Loss Cls: 0.34804
[eval] Epoch 60 — acc: 0.5560, macro_f1: 0.5559

[train] ===== Época 61/100 =====
[train] acc: 0.946 | Loss DDPM: 0.31956 | Loss Gap: 0.19761 | Loss Cls: 0.24756
[eval] Epoch 61 — acc: 0.5680, macro_f1: 0.5671
[checkpoint] ¡Nueva mejor accuracy: 0.5680! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 62/100 =====
[train] acc: 0.948 | Loss DDPM: 0.31832 | Loss Gap: 0.19656 | Loss Cls: 0.21906
[eval] Epoch 62 — acc: 0.5587, macro_f1: 0.5583

[train] ===== Época 63/100 =====
[train] acc: 0.950 | Loss DDPM: 0.32224 | Loss Gap: 0.20420 | Loss Cls: 0.24961
[eval] Epoch 63 — acc: 0.5667, macro_f1: 0.5659

[train] ===== Época 64/100 =====
[train] acc: 0.932 | Loss DDPM: 0.34697 | Loss Gap: 0.25131 | Loss Cls: 0.37080
[eval] Epoch 64 — acc: 0.5333, macro_f1: 0.5330

[train] ===== Época 65/100 =====
[train] acc: 0.920 | Loss DDPM: 0.33050 | Loss Gap: 0.22483 | Loss Cls: 0.45010
[eval] Epoch 65 — acc: 0.5587, macro_f1: 0.5594

[train] ===== Época 66/100 =====
[train] acc: 0.946 | Loss DDPM: 0.31969 | Loss Gap: 0.19874 | Loss Cls: 0.28295
[eval] Epoch 66 — acc: 0.5467, macro_f1: 0.5466

[train] ===== Época 67/100 =====
[train] acc: 0.946 | Loss DDPM: 0.31587 | Loss Gap: 0.19475 | Loss Cls: 0.22037
[eval] Epoch 67 — acc: 0.5507, macro_f1: 0.5505

[train] ===== Época 68/100 =====
[train] acc: 0.957 | Loss DDPM: 0.31522 | Loss Gap: 0.19522 | Loss Cls: 0.19712
[eval] Epoch 68 — acc: 0.5653, macro_f1: 0.5648

[train] ===== Época 69/100 =====
[train] acc: 0.956 | Loss DDPM: 0.33186 | Loss Gap: 0.22112 | Loss Cls: 0.25343
[eval] Epoch 69 — acc: 0.5333, macro_f1: 0.5332

[train] ===== Época 70/100 =====
[train] acc: 0.923 | Loss DDPM: 0.34097 | Loss Gap: 0.24393 | Loss Cls: 0.45330
[eval] Epoch 70 — acc: 0.5307, macro_f1: 0.5308

[train] ===== Época 71/100 =====
[train] acc: 0.955 | Loss DDPM: 0.31731 | Loss Gap: 0.20079 | Loss Cls: 0.30641
[eval] Epoch 71 — acc: 0.5453, macro_f1: 0.5455

[train] ===== Época 72/100 =====
[train] acc: 0.959 | Loss DDPM: 0.31517 | Loss Gap: 0.19476 | Loss Cls: 0.21075
[eval] Epoch 72 — acc: 0.5507, macro_f1: 0.5500

[train] ===== Época 73/100 =====
[train] acc: 0.960 | Loss DDPM: 0.31382 | Loss Gap: 0.19321 | Loss Cls: 0.17180
[eval] Epoch 73 — acc: 0.5560, macro_f1: 0.5556

[train] ===== Época 74/100 =====
[train] acc: 0.969 | Loss DDPM: 0.32089 | Loss Gap: 0.20318 | Loss Cls: 0.18916
[eval] Epoch 74 — acc: 0.5560, macro_f1: 0.5556

[train] ===== Época 75/100 =====
[train] acc: 0.942 | Loss DDPM: 0.33451 | Loss Gap: 0.23256 | Loss Cls: 0.31753
[eval] Epoch 75 — acc: 0.5533, macro_f1: 0.5527

[train] ===== Época 76/100 =====
[train] acc: 0.941 | Loss DDPM: 0.33242 | Loss Gap: 0.22640 | Loss Cls: 0.35946
[eval] Epoch 76 — acc: 0.5387, macro_f1: 0.5385

[train] ===== Época 77/100 =====
[train] acc: 0.963 | Loss DDPM: 0.32022 | Loss Gap: 0.19919 | Loss Cls: 0.22901
[eval] Epoch 77 — acc: 0.5547, macro_f1: 0.5547

[train] ===== Época 78/100 =====
[train] acc: 0.964 | Loss DDPM: 0.31448 | Loss Gap: 0.19390 | Loss Cls: 0.16410
[eval] Epoch 78 — acc: 0.5640, macro_f1: 0.5644

[train] ===== Época 79/100 =====
[train] acc: 0.976 | Loss DDPM: 0.31374 | Loss Gap: 0.19372 | Loss Cls: 0.15230
[eval] Epoch 79 — acc: 0.5413, macro_f1: 0.5409

[train] ===== Época 80/100 =====
[train] acc: 0.965 | Loss DDPM: 0.32494 | Loss Gap: 0.21607 | Loss Cls: 0.19336
[eval] Epoch 80 — acc: 0.5560, macro_f1: 0.5555

[train] ===== Época 81/100 =====
[train] acc: 0.942 | Loss DDPM: 0.33705 | Loss Gap: 0.24100 | Loss Cls: 0.37532
[eval] Epoch 81 — acc: 0.5280, macro_f1: 0.5284

[train] ===== Época 82/100 =====
[train] acc: 0.955 | Loss DDPM: 0.32153 | Loss Gap: 0.20376 | Loss Cls: 0.29216
[eval] Epoch 82 — acc: 0.5547, macro_f1: 0.5543

[train] ===== Época 83/100 =====
[train] acc: 0.970 | Loss DDPM: 0.30915 | Loss Gap: 0.19124 | Loss Cls: 0.17650
[eval] Epoch 83 — acc: 0.5533, macro_f1: 0.5531

[train] ===== Época 84/100 =====
[train] acc: 0.971 | Loss DDPM: 0.31296 | Loss Gap: 0.19260 | Loss Cls: 0.13534
[eval] Epoch 84 — acc: 0.5533, macro_f1: 0.5532

[train] ===== Época 85/100 =====
[train] acc: 0.976 | Loss DDPM: 0.31406 | Loss Gap: 0.19657 | Loss Cls: 0.14562
[eval] Epoch 85 — acc: 0.5573, macro_f1: 0.5569

[train] ===== Época 86/100 =====
[train] acc: 0.961 | Loss DDPM: 0.32899 | Loss Gap: 0.23049 | Loss Cls: 0.25393
[eval] Epoch 86 — acc: 0.5360, macro_f1: 0.5366

[train] ===== Época 87/100 =====
[train] acc: 0.938 | Loss DDPM: 0.32944 | Loss Gap: 0.22141 | Loss Cls: 0.40097
[eval] Epoch 87 — acc: 0.5387, macro_f1: 0.5398

[train] ===== Época 88/100 =====
[train] acc: 0.967 | Loss DDPM: 0.31447 | Loss Gap: 0.19602 | Loss Cls: 0.20090
[eval] Epoch 88 — acc: 0.5467, macro_f1: 0.5471

[train] ===== Época 89/100 =====
[train] acc: 0.970 | Loss DDPM: 0.31343 | Loss Gap: 0.19304 | Loss Cls: 0.14460
[eval] Epoch 89 — acc: 0.5600, macro_f1: 0.5602

[train] ===== Época 90/100 =====
[train] acc: 0.976 | Loss DDPM: 0.31001 | Loss Gap: 0.19118 | Loss Cls: 0.11895
[eval] Epoch 90 — acc: 0.5573, macro_f1: 0.5578

[train] ===== Época 91/100 =====
[train] acc: 0.975 | Loss DDPM: 0.31874 | Loss Gap: 0.20847 | Loss Cls: 0.16101
[eval] Epoch 91 — acc: 0.5413, macro_f1: 0.5415

[train] ===== Época 92/100 =====
[train] acc: 0.951 | Loss DDPM: 0.33561 | Loss Gap: 0.23673 | Loss Cls: 0.31097
[eval] Epoch 92 — acc: 0.5200, macro_f1: 0.5202

[train] ===== Época 93/100 =====
[train] acc: 0.962 | Loss DDPM: 0.31811 | Loss Gap: 0.20698 | Loss Cls: 0.27075
[eval] Epoch 93 — acc: 0.5533, macro_f1: 0.5533

[train] ===== Época 94/100 =====
[train] acc: 0.975 | Loss DDPM: 0.31164 | Loss Gap: 0.19256 | Loss Cls: 0.15278
[eval] Epoch 94 — acc: 0.5533, macro_f1: 0.5537

[train] ===== Época 95/100 =====
[train] acc: 0.976 | Loss DDPM: 0.31070 | Loss Gap: 0.19101 | Loss Cls: 0.11204
[eval] Epoch 95 — acc: 0.5600, macro_f1: 0.5604

[train] ===== Época 96/100 =====
[train] acc: 0.978 | Loss DDPM: 0.31009 | Loss Gap: 0.19363 | Loss Cls: 0.12352
[eval] Epoch 96 — acc: 0.5707, macro_f1: 0.5707
[checkpoint] ¡Nueva mejor accuracy: 0.5707! Checkpoint guardado en /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt

[train] ===== Época 97/100 =====
[train] acc: 0.966 | Loss DDPM: 0.32449 | Loss Gap: 0.21753 | Loss Cls: 0.18171
[eval] Epoch 97 — acc: 0.5133, macro_f1: 0.5124

[train] ===== Época 98/100 =====
[train] acc: 0.948 | Loss DDPM: 0.32596 | Loss Gap: 0.22483 | Loss Cls: 0.35700
[eval] Epoch 98 — acc: 0.5560, macro_f1: 0.5563

[train] ===== Época 99/100 =====
[train] acc: 0.972 | Loss DDPM: 0.31226 | Loss Gap: 0.19460 | Loss Cls: 0.19944
[eval] Epoch 99 — acc: 0.5493, macro_f1: 0.5492

[train] ===== Época 100/100 =====
[train] acc: 0.974 | Loss DDPM: 0.31104 | Loss Gap: 0.19179 | Loss Cls: 0.11872
[eval] Epoch 100 — acc: 0.5693, macro_f1: 0.5694

[test] Evaluando el mejor modelo en el conjunto de test…
[test] Checkpoint restaurado desde /media/beegfs/home/w314/w314139/PROJECT/silent-speech-decoding/experiments/DiffE_original_BCI_raw_ALL/best.pt
[test] Reporte completo:

  metrics:
            accuracy: 0.5187
    balanced_accuracy: 0.5187
            macro_f1: 0.5184
     macro_precision: 0.5187
        macro_recall: 0.5187
         roc_auc_ovo: 0.7988
                 mcc: 0.3984
         cohen_kappa: 0.3983
    confusion_matrix: [[74 13 20 23 20]
 [17 83 21 15 14]
 [18 20 73 17 22]
 [11 20 20 76 23]
 [23 14 13 17 83]]

  baseline_random:
            accuracy: 0.2000
    balanced_accuracy: 0.2011
            macro_f1: 0.2010
     macro_precision: 0.2013
        macro_recall: 0.2011
         roc_auc_ovo: 0.5000
                 mcc: 0.0014
         cohen_kappa: 0.0014

  improvement_%:
            accuracy: 159.3333
    balanced_accuracy: 157.8550
            macro_f1: 157.9603
     macro_precision: 157.6439
        macro_recall: 157.8550
         roc_auc_ovo: 59.7689
                 mcc: 27676.3506
         cohen_kappa: 27690.6977

  confusion_matrix:
[[74 13 20 23 20]
 [17 83 21 15 14]
 [18 20 73 17 22]
 [11 20 20 76 23]
 [23 14 13 17 83]]
